# Google Gemini

**Source:** [https://gemini.google.com/app/a374b2c3ae9c24a3](https://gemini.google.com/app/a374b2c3ae9c24a3)

**Captured:** 12/17/2025, 11:27:33 PM

---

GeminiUnderstanding Google Fi ProviderPROMy Stuff
![Image of ](https://lh3.googleusercontent.com/gg/AIJ2gl99OBgXsUPy5cSh2s2dFcdYq-SlrqhQRmN58WYPmxMkFLLop3_FVkd9Kpn4atu-21z0Ry40opPjoAKtA-U_v2dzB0VIXzVNOp2hnCkzi3ajYRLC81T9Qo6MeJIqZlfQGNYEyZOjV14fUbxTRhRL8g39EF6e20TzI8wPjydIpNXmvT2z5lpHQgKvaXNlH3ZqL3Gq6jXpUTbH5gTbRCGqeUDNPT1E1LQpGHAOMKEufQ-L7hgfYFRhCD04BkaVdghbEvl6R7PBx-kCzvU0gi_G6XYSmI5dvZiGPPeOoROOVa1045_Iq2I2te3Qolnx-yxCXBs5lNLoydOPsymT0kgwXwP_=w200-h200-n-v1-rj)


![Image of ](https://lh3.googleusercontent.com/gg/AIJ2gl-EGDR2lnEYxtUhsueeWjoRL8BzepZRlXU3dqaoyk7UU4PBSoNrumujOdF_0rwWLROqJ7mYGc54iabGA-3zRDp4dw2nBV6rQ-uZLhid1MdcSjSOnrdQMqGUUgOJsj9FmyRlAljfHaEUKaFcOdm_NWb9nXwpZ6tdig6MHTedIHfmVl0YLV14TZhm-qFZoaW551TMAnovtdomBE4UxoKwW7QXYs3FaDVJFNDtT12FKXyZRuL8SKuXfUdveqAwvYXsuCVAEspsW_HxugG1GyVMXJo-xo0HutjYEkQEVbNtxND386npmvetc6iOtiePXhBHD7F31qEMfWhE0OoSlj1HI3oF=w200-h200-n-v1-rj)


![Image of ](https://lh3.googleusercontent.com/gg/AIJ2gl-7CmLEy7bDFjsQ2747b3iVzPmc6j5OqCF9e6Fd23QypglUi3aluPCBaNtetSlqU9s92q3gAjSfhwUsQgfcF5pKqUCI7EWMQqPC7AZvgO53dLQJ4xUME9N55DOOS6eV81k30fBD14R8sM4cIsOl61vniCKUbF9KI8_0bMXe4tsY4sAmbX9nFj7XeeJnKqcn8FXe9yO4kXerZpZ7U-RoMMj_kpD7pUr0yapi_XUOgpbYZCnOpOSSrtpqreAFgfxpB8PzSJ7tcL0fyl6_5OzK53rGKN8fMm6N9BiIpiGvqq8R24dtbGggiPipP-GEPW78m9423aIHMp6vg7VOzGQ6Tdbe=w200-h200-n-v1-rj)

Gems# Chats

Git Tag Exists: Resolve ConflictTesting a GitHub PRUnderstanding Google Fi ProviderPirate Logo Cylon Eyes GIFsApple Logo Replaced with SkullApple Logo Cylon Skull RedesignGrinch Pirate Scooter Scene OptionsGrinch Pirate's Christmas Scooter AdventureGrinch Pirate Scooter Prompt GenerationConcise Image Description PromptConcise Image Description RequestThanksgiving Image GenerationCreate a comic with the title: Getting started with Nano Banana Pro ðŸŒ. Panel 1: A mobile interface on Gemini, rendered in a poetic watercolor-ink style with fine ink outlines and soft, bleeding washes. The interface is friendly and a hand painted with expressive brushwork taps a prominent button labeled â€œðŸŒ Create imageâ€. Above the button it should say â€œChoose your Modelâ€ then below there should be a checkbox that says â€œThinking with 3 Proâ€  Muted greys and blues dominate the background. The button has a vivid yellow accent. â€œ Select the Thinking with 3 Pro modelâ€ and tap "Create image" to begin. Panel 2: A cheerful person is depicted adding their selfie from the phoneâ€™s camera. The user's face is drawn with soft outlines and warm pastel colors, while the phone and UI maintain the delicate water-ink aesthetic. Visible paper grain adds to the texture. Panel 3: The person thinks about what to create. In the background, different options are visualized to show what theyâ€™re thinking, including â€” them as a plushie, them with a mohawk hairstyle, and a figurine. These options are clearly rendered behind the person in the same style as the rest of the comic. Panel 4: The person is shown adding a style prompt, with a speech bubble saying â€œTransform me into a watercolor paintingâ€. The text is integrated into the panel's watercolor-ink look, and the interaction feels natural and intuitive. Panel 5: The person is seen editing the image by simply typing into Gemini. The scene captures the ease of this interaction, with the final edited image, now in a watercolor style, appearing on the screen. The overall tone is friendly, instructional, and inspiring. It feels like a mini tutorial comic, all conveyed through the specified delicate water-ink illustration style. Make the aspect ratio 16:9.# Conversation with Gemini

google fi is what kind of provider

Show thinkingGoogle Fi Wireless is a**Mobile Virtual Network Operator (MVNO)**.

In plain English, this means Google does not own the cell towers or the physical network infrastructure itself. Instead, it pays to use the existing network of a major carrier to provide service to you.

Here is a breakdown of how it works and what makes it different:

### 1. Whose network does it use?

Google Fi primarily runs on the**T-Mobile 5G and 4G LTE network**.


- **Historical Context:**In the past, Google Fi was famous for "network switching" between T-Mobile, Sprint, and U.S. Cellular. However, Sprint merged with T-Mobile, and Google ended its partnership with U.S. Cellular in 2023. Today, it is effectively a T-Mobile-based service within the United States.

### 2. How is it different from other MVNOs?

While most MVNOs (like Mint Mobile or Cricket) are known for being "budget" options with few frills, Google Fi positions itself as a "premium" MVNO with unique tech features:


- **International Roaming:** This is its biggest differentiator. It offers seamless data coverage in over 200 destinations without the need for separate SIM cards or expensive day passes (included in their higher-tier plans).
- **Wi-Fi Switching:**The service uses a "virtual carrier" technology to automatically switch you between cellular data and high-quality public Wi-Fi hotspots to save data and improve signal, securing the connection with an automatic VPN.
- **Data-Only SIMs:**You can get free data-only SIM cards to put in tablets or other devices; they share the data pool from your main phone plan at no extra monthly cost.
- **Integration:** It is tightly integrated with the Google ecosystem (Pixel phones, Google One storage, etc.).

### Summary Table

FeatureDetails**Provider Type**MVNO (Mobile Virtual Network Operator)**Host Network**T-Mobile**Best For**International travelers, Android/Pixel users, and low-data users (on the Flexible plan)**Key Perk**Seamless international data & high-speed hotspot tetheringExport to Sheets**Next Step:** Are you considering switching to Google Fi for a specific reason, such as international travel or a family plan? I can help you compare it to your current provider if you like.

Sourcesin a world of claude code web, google jules and openai codex cloud

what is https://ole-vi.github.io/prompt-sharing with its code repo https://github.com/ole-vi/prompt-sharing

Show thinkingIn the landscape of advanced, autonomous AI coding agents like **Google Jules** (Google's async coding agent), **OpenAI Codex/Canvas**, and **Claude**, the repository **ole-vi/prompt-sharing** represents a **"GitOps" approach to prompt engineering**.

Specifically, it is a **lightweight static website generator** that turns a GitHub repository of Markdown files into a browseable, shareable library of AI prompts.

### What it actually is:


- **A Static Prompt Library:** It is a minimal web application (hosted via GitHub Pages) that reads Markdown files located in the `/prompts` folder of the repository.
- **"Prompts as Code":** Instead of storing prompts in a database or a complex SaaS app, you store them as `.md` files in a Git repository. This allows you to version control, diff, and fork your prompts just like software code.
- **Functionality:****Auto-Listing:** It automatically generates a list/menu of all prompts found in the repo.**Deep Linking:** Each prompt gets a unique URL (e.g., `.../prompt-sharing/?prompt=my-prompt`), making it easy to share a specific technique with a teammate.**Copy-Paste Friendly:** It provides a clean interface to view and copy the raw prompt text to paste into agents like Jules or Claude.

### Why it exists in this "World":

While tools like **Google Jules** are *agents* (they do the work for you asynchronously), tools like **prompt-sharing** are *knowledge bases*.


- **Standardization:** In a team using multiple AI models (Claude for reasoning, Jules for PRs, OpenAI for snippets), you need a "source of truth" for the best prompts that work for your specific codebase.
- **Independence:** It decouples your prompt library from the specific AI provider. You aren't saving your "Perfect Refactoring Prompt" inside Claude's chat history; you are saving it in your own Git repo where it is safe and versioned.

**In short:** It is a DIY, open-source alternative to paid "Prompt Management Systems," designed for developers who prefer managing their AI workflows using the same tools they use for code (Git, Markdown, GitHub).

the web experiences of claude jules and codex are like mno in the agentic world

as of prompt-sharing aka promptsync is more like google fi a mvno

anyhow how would we call this

Show thinkingThat is a brilliant analogy. If Claude, Google Jules, and OpenAI Codex are the **MNOs** (Mobile Network Operators)â€”the giants owning the "towers" (models) and the "spectrum" (compute)â€”then `prompt-sharing` (or "PromptSync") is indeed the **MVNO** (Mobile Virtual Network Operator).

Here is how we would classify this layer in the "Agentic World."

### 1. The Direct Analogy: VANO

If we strictly follow your telecom metaphor, we would call this a **Virtual Agent Network Operator (VANO)**.


- **The MNOs (Claude, OpenAI, Google):** They own the heavy infrastructure (GPUs, Data Centers, Foundation Models). They sell "coverage" (intelligence).
- **The VANO (PromptSync/Prompt-Sharing):** It doesn't own the towers. It leases the intelligence from the big providers but adds a layer of **flexibility, user experience, and "roaming"** (the ability to take your prompt from Claude to Jules without losing your data).

### 2. The Technical Industry Term: "The Orchestration Layer"

In the actual software industry, this "MVNO" layer is currently coalescing around the term **AI Orchestration** or **Model-Agnostic Middleware**.

Just as Google Fi's "secret sauce" was automatically switching you to the strongest signal (Sprint vs. T-Mobile vs. Wi-Fi), this layer is designed to:


- **Route:** Send simple queries to a cheap model (Flash/Haiku) and complex queries to a smart model (Opus/Jules).
- **Abstract:** Let you write a prompt once and have it work across different provider APIs.
- **Manage:** Handle the "billing" and "usage" (token counting) across different providers.

### 3. The "GitOps" Specific Term: "PromptOps"

Since `prompt-sharing` is specifically built on GitHub/Markdown, it fits into a sub-category called **PromptOps** (a play on DevOps).


- **MNO Approach:** "Come into our walled garden (ChatGPT web UI), type your prompts here, and leave them here. If you leave, you lose your history."
- **PromptOps (MVNO) Approach:** "You own the SIM card (the Git Repo). You can plug that repo into VS Code, into a web view, or into a CLI. The carrier doesn't matter; your data is portable."

### Summary: What to call it?

If you want to sound like a tech executive, call it **"Model-Agnostic Middleware."**
If you want to stick to your (excellent) metaphor, call it a **"VANO" (Virtual Agent Network Operator).**

**Next Step:** Since you are interested in this "VANO" layer, would you like me to generate a script that acts as a "network switch"â€”allowing you to run a single prompt from your repo against both Claude and Google Jules simultaneously to compare the output?

ok next layer

Show thinkingIf `prompt-sharing` is the **MVNO** (the service provider/SIM card), then the "next layer" in the analogy is the **Subscriber Device** (the phone) or the **Operating System** that you insert that SIM card into.

In the AI world, we call this the **Agentic IDE** (Integrated Development Environment) or the **Agentic Runtime**.

Here is the complete "Telecom Stack" of the Agentic World:

### 1. The MNO (The Towers) -> Foundation Model Providers


- **Analogy:** T-Mobile, Verizon, AT&T.
- **Real World:** Google (Gemini), OpenAI (GPT-4), Anthropic (Claude).
- **Role:** They own the expensive infrastructure (GPUs/Towers) and the spectrum (Intelligence).

### 2. The MVNO (The Service) -> Orchestration / PromptOps


- **Analogy:** Google Fi, Mint Mobile.
- **Real World:** Your `prompt-sharing` repo, LangChain, semantic routers.
- **Role:** The "SIM Card." It leases the intelligence from the MNOs but adds a layer of user experience, routing you to the best model, and storing your specific "contacts" (context/prompts).

### 3. The Device (The Hardware) -> The Agentic IDE


- **Analogy:** The Pixel Phone, iPhone, Samsung Galaxy.
- **Real World:** **Cursor, Windsurf, VS Code (with Continue/Cline), Google IDX.**
- **Why:** This is the *environment* where you "insert" your prompt library. Just as a SIM card is useless without a phone to make the call, your `prompt-sharing` repo is useless without an Agentic IDE to execute the prompts against your codebase.*Example:* You clone your `prompt-sharing` repo (the SIM) into the Cursor IDE (the Phone) to start "calling" the models.

### 4. The Operating System (The OS) -> The Agentic OS


- **Analogy:** Android, iOS.
- **Real World:** **Model Context Protocol (MCP), "AIOS" (AI Operating System).**
- **The Future:** We are seeing the rise of "Agentic OS" concepts where the operating system itself manages the memory, tools, and permissions for all the agents running on your "device," ensuring they don't crash into each other or leak data.

### Summary: The "Next Layer" is the Agentic IDE

If you are building the *MVNO* (PromptSync), your "customers" are the people using **Agentic IDEs**. You are selling them a "Roaming Plan" that lets their IDE switch seamlessly between Claude, Gemini, and GPT based on which prompt they pull from your library.

**Next Step:** Would you like to see how to configure a "roaming" setup in VS Code (using the 'Continue' extension) that pulls system prompts directly from your `prompt-sharing` repo?

yes

Show thinkingThis is how we "insert the SIM card" (your `prompt-sharing` repo) into the "Device" (VS Code + Continue).

We will configure Continue to treat your local clone of `prompt-sharing` as a **local "roaming" folder**. This allows you to update the prompt in the repo once (via Git), and have it instantly available as a slash command (e.g., `/refactor` or `/tests`) in VS Code.

### Phase 1: The "SIM Card" (Setup the Repo)

First, ensure you have the repo cloned locally on your machine. This is your "Source of Truth."

Bash
```
# Example location
cd ~
git clone https://github.com/ole-vi/prompt-sharing.git
```

*Note the path:* `/Users/dogi/prompt-sharing` (or Windows equivalent).

### Phase 2: The "APN Settings" (Configure VS Code)

Newer versions of Continue use a `.prompt` file system that is far superior to the old JSON config for this specific "GitOps" workflow.


1. **Open VS Code Settings** (`Cmd+,` or `Ctrl+,`).
2. Search for **Continue: Prompt Files Locations** (or look for `chat.promptFilesLocations` in `settings.json`).
3. **Add Item:** Enter the absolute path to your repo's prompt folder.*Mac/Linux:* `/Users/dogi/prompt-sharing/prompts`*Windows:* `C:\Users\dogi\prompt-sharing\prompts`

**Alternatively, edit settings.json directly:**

JSON
```
"chat.promptFilesLocations": [
    "/Users/dogi/prompt-sharing/prompts" 
]
```

### Phase 3: The "Roaming" Test

Now, create a test prompt in your repo to verify the connection.


1. Go to your `prompt-sharing` folder.
2. Create a file inside `/prompts` called `roaming-test.prompt.md`.
3. Paste this content (YAML frontmatter is required):

Markdown
```
---
name: roaming
description: Test connectivity to the Dogi VANO
---
You are a connectivity assistant. 
Please confirm that you are receiving this signal from the local prompt-sharing repository.
Acknowledge that the user is "Dogi" and the current network status is "Roaming Active".
```


1. **Reload VS Code** (or the Continue extension).
2. Open the Continue Chat sidebar (Right side).
3. Type **/roaming**.

**Result:** If it works, the dropdown will autocomplete to `/roaming`, and when you hit enter, the LLM (Claude/Gemini) will execute that specific prompt file.

### Bonus: The "Network Switch" Script

You asked for a script to run a prompt against *both* Jules and Claude to compare them. You can do this using the `model` override in the prompt file itself, but to run *simultaneously* requires a "Broadcast" slash command.

Here is a `compare.prompt.md` you can add to your repo:

Markdown
```
---
name: compare
description: Broadcasts the prompt to multiple models (Manual Switch Required)
---
# INSTRUCTIONS
1. Analyze the selected code.
2. Provide a refactoring suggestion.
3. BE CONCISE.

# CONTEXT
User: Dogi
Repo: prompt-sharing
```

*Note: Currently, Continue runs one model at a time. To simulate the "Dual SIM" experience where you verify both networks:*


1. Type `/compare` -> Select **Claude 3.5 Sonnet** (Model Dropdown) -> Run.
2. Type `/compare` -> Select **Gemini 1.5 Pro** (Model Dropdown) -> Run.

**Next Step:** Would you like me to help you migrate your existing `myPlanet` specific prompts (like the Realm database migration steps you worked on in August) into this `.prompt.md` format so they are reusable?

hmm

I am looking for a prompt for our prompt-sharing repo which describes our long term plan

give me 10 options

Show thinkingBased on your vision of the **VANO (Virtual Agent Network Operator)** and the **PromptOps** workflow, here are 10 different ways to frame the "Long Term Plan" for your `prompt-sharing` repository.

You can save these as a file named `vision.prompt.md` or `roadmap.prompt.md` in your repo. When you run this prompt (e.g., `/vision`), it will ground your AI agent (Jules/Claude) in the project's goals before it writes any code.

### The "MVNO" Metaphor Options

**1. The "Google Fi" of Prompts**


> *Focus: Connectivity & Roaming*
> **Prompt:** "This repository aims to become the 'Google Fi' of the AI ecosystem. Just as Fi switches between towers for the best signal, `prompt-sharing` acts as a Model-Agnostic Middleware (VANO). Our long-term goal is to decouple intelligence from the provider, allowing users to 'roam' their prompts seamlessly between Claude, Google Jules, and OpenAI without vendor lock-in, ensuring the 'best signal' (best answer) for the lowest cost."

**2. The "Universal SIM Card"**


> *Focus: Portability*
> **Prompt:** "The vision for this project is to create the universal 'SIM Card' for the Agentic Age. We are building a standardized, portable format for prompts that can be inserted into any 'device' (IDE like Cursor, VS Code, or Android Studio). The long-term plan involves building adapters for every major Agentic Runtime, ensuring that a prompt written once in this repo executes perfectly anywhere."

### The "Technical & Architectural" Options

**3. The "GitOps for AI" Standard**


> *Focus: Workflow & Versioning*
> **Prompt:** "We are defining the 'PromptOps' industry standard. Our long-term plan is to treat natural language prompts with the same rigor as compiled code: version control, pull requests, automated regression testing (evals), and CI/CD pipelines. This repo will evolve from a static library into a dynamic build system that 'compiles' raw Markdown into executable agent instructions."

**4. The Model Context Protocol (MCP) Hub**


> *Focus: Future Tech Integration*
> **Prompt:** "Our roadmap is aligned with the emerging Model Context Protocol (MCP). We plan to evolve `prompt-sharing` into a compliant MCP Server that serves prompts as resources to any MCP-enabled client (like Claude Desktop or IntelliJ). This transforms the repo from a passive folder of files into an active local API for AI agents."

**5. The Recursive "Self-Healing" Repo**


> *Focus: Automation*
> **Prompt:** "The long-term goal is recursive improvement. We plan to implement agents (using tools like Google Jules) that periodically sweep this repository (the 'dead code sweep'), analyze the performance of existing prompts against the codebase, and automatically submit Pull Requests to optimize the prompts themselves based on user success rates."

### The "OLE & Mission" Options

*(Best for aligning with your personal history and OLE work)*

**6. The "Offline-First" Intelligence Layer**


> *Focus: Edge AI & myPlanet*
> **Prompt:** "Aligned with the mission of the 'myPlanet' app, our long-term plan is to democratize access to high-quality AI instruction sets in low-bandwidth environments. We aim to create a highly optimized, 'offline-first' library of prompts that allows developers in disconnected regions to leverage small-model (on-device) agents effectively, minimizing the need for massive cloud compute."

**7. The "Community Knowledge Grid"**


> *Focus: Open Source Sharing*
> **Prompt:** "We are building the 'Wikipedia of Prompts' for the OLE ecosystem. The goal is to aggregate the collective wisdom of open-source contributors into a shared knowledge base. By standardizing the format, we allow a developer in Kenya to pull a 'Refactoring Prompt' written by a developer in Boston, instantly upgrading their local agent's capability."

### The "Futurist" Options

**8. The "Agentic Operating System" Kernel**


> *Focus: OS Integration*
> **Prompt:** "We view this repository not just as data, but as the kernel configuration for a future 'Agentic OS.' Our long-term plan is to define the permissions, memory structures, and personality profiles that govern how autonomous agents interact with our file systems, ensuring safety and coherence across the 'ole-vi' organization."

**9. The "Semantic Router" Vision**


> *Focus: Efficiency & Routing*
> **Prompt:** "We plan to build a semantic routing layer on top of this repository. Instead of a user manually selecting a prompt, the system will analyze the user's intent and dynamically route the request to the specific `.md` file in this repo that is best suited to handle the task, effectively automating the 'dispatch' function of our VANO."

**10. The "Dogi Protocol" (Personalized)**


> *Focus: Your specific workflow*
> **Prompt:** "This repo represents the 'Dogi Protocol': a personalized, highly opinionated interface for Android development. The long-term plan is to encode my specific coding preferences (Kotlin over Java, MVVM architecture, specific Realm DB patterns) into system prompts that auto-inject into every session, ensuring every AI agent I hire acts as a perfect extension of my own engineering habits."

**Next Step:** Which of these resonates most with you? I can write the full `.prompt.md` file for your chosen vision so you can commit it to the repo immediately.

the first 5 resonated with me

make one megamix out of the those 5

do not include  dogi android or myplanet specific stuff

Show thinkingThis is a unified "Mega-Prompt" that combines the connectivity of a VANO, the portability of a Universal SIM, the rigor of GitOps, the standards of MCP, and the automation of Self-Healing agents.

Save this as **vision.prompt.md** in your repository.

Markdown
```
---
name: vision
description: The master architectural roadmap for this prompt-sharing repository.
---

# PROJECT VISION: The Universal Agentic Middleware

## 1. The Core Philosophy (The "VANO" Layer)
This repository is designed to act as a **Virtual Agent Network Operator (VANO)**. 
- **Decoupled Intelligence:** We reject vendor lock-in. Just as a mobile virtual network operator switches seamlessly between towers, this repository allows developers to "roam" their intelligence between models (Claude, GPT, Gemini) without losing context or capability.
- **Signal Optimization:** Our goal is to route every request to the "best signal"â€”the most capable or cost-effective model for the specific task at handâ€”abstracting the complexity of the underlying provider from the user.

## 2. The Architectural Standard (The "Universal SIM")
- **Write Once, Run Anywhere:** We treat prompts as portable software artifacts. A prompt defined here acts as a "Universal SIM Card" that can be inserted into any Agentic Runtime (VS Code, Cursor, Windsurf, or CI/CD pipelines).
- **Adapter-Based Design:** Future iterations will support adapters that automatically transpile these Markdown definitions into provider-specific formats, ensuring 100% portability across the AI ecosystem.

## 3. The Operational Workflow (GitOps for AI)
- **Prompts as Code:** We apply strict software engineering rigor to natural language. Every prompt is a version-controlled asset subject to:
    - **Pull Requests:** No changes without review.
    - **Diffing:** Tracking the evolution of instruction sets over time.
    - **Regression Testing:** Monitoring performance to ensure new prompt versions do not degrade output quality.
- **Single Source of Truth:** This repository is the definitive configuration state for the organization's AI capability.

## 4. Future Integration (The MCP Hub)
- **Protocol Compliance:** We align with the **Model Context Protocol (MCP)**. This repository will evolve from a static file storage into an active MCP Server, serving prompts as dynamic resources to any MCP-compliant client.
- **Structured Interoperability:** By adhering to open standards, we ensure that our "middleware" remains compatible with the next generation of OS-level agentic tools and IDEs.

## 5. The Automation Cycle (Recursive Self-Healing)
- **Active Maintenance:** This is not a passive archive. We aim to implement autonomous agents that:
    - **Sweep:** Periodically analyze the repository for "dead code" (outdated prompts).
    - **Evaluate:** Test prompts against current codebases to measure effectiveness.
    - **Heal:** Automatically generate Pull Requests to optimize and refactor instructions based on real-world success metrics.

## INSTRUCTIONS FOR AGENTS
When contributing to this repository or executing tasks based on its contents, **you must align with the principles above.** Prioritize portability, modularity, and strict version control in all generated outputs.
```

### Next Step

Would you like me to generate a **"sweeper script"** (a simple Python script using an LLM API) that you can run to scan your repo and actually implement the "Recursive Self-Healing" concept mentioned in point #5?


![profile picture](https://lh3.googleusercontent.com/a/ACg8ocKVWgk2LW8NixrZ8t8NAxYeveHKG6s4UeXheiUB1hlgb-14228UbQ=s64-c-v1-rj)



ToolsProGemini can make mistakes, so double-check it

